{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습] Fashion MNIST 학습하기 <a class='tocSkip'>\n",
    "    \n",
    "## 목표 : 새로운 CNN모델을 만들어 최고 accuracy를 달성해주세요. <a class='tocSkip'>\n",
    "\n",
    "## 제한시간 : 20분 <a class='tocSkip'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def get_data():\n",
    "    return input_data.read_data_sets(\"datasets/fashion\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST <a class='tocSkip'>\n",
    "* MNIST데이터 셋과 동일한 category수를 가지고 있으나 손글씨 숫자가 아닌 fashion item이미지로 구성\n",
    "* Fashion MNIST데이터의 category label은 다음과 같이 정의 되어 있음\n",
    "\n",
    "| Label\t| Description |\n",
    "| --- | --- |\n",
    "|0\t|T-shirt/top|\n",
    "|1\t|Trouser |\n",
    "|2\t|Pullover|\n",
    "|3\t|Dress|\n",
    "|4\t|Coat|\n",
    "|5\t|Sandal|\n",
    "|6\t|Shirt|\n",
    "|7\t|Sneaker|\n",
    "|8\t|Bag|\n",
    "|9\t|Ankle boot|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/fashion/t10k-labels-idx1-ubyte.gz\n",
      "1 1 (784,)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fashion_mnist = get_data()\n",
    "test_images, test_labels = fashion_mnist.test.next_batch(1)\n",
    "print len(test_images), len(test_labels), test_images[0].shape\n",
    "plt.imshow(np.reshape(test_images[0], (28, 28)), cmap='gray')\n",
    "print test_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder : graph 입력부분 정의\n",
    "* <span class=\"burk\">placeholder 부분을 정의해주세요.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "solution2": "shown"
   },
   "source": [
    "```python\n",
    "def get_inputs():\n",
    "    x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    return x, y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model : algorithm 을 graph 연산으로 정의\n",
    "* <span class=\"burk\">모델을 자유롭게 구성해주세요.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def build_model(images):\n",
    "\n",
    "  x_image = tf.reshape(images, [-1, 28, 28, 1])\n",
    "\n",
    "  # filter shape : w, h, in_channel, out_channel\n",
    "  conv1_filters = tf.Variable(tf.random_normal([3, 3, 1, 16], stddev=0.01))\n",
    "  conv1 = tf.nn.conv2d(x_image, conv1_filters, strides=[1, 1, 1, 1], padding='SAME')\n",
    "  conv1 = tf.nn.relu(conv1)\n",
    "  print 'conv1', conv1\n",
    "\n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "  print 'pool1', pool1\n",
    "\n",
    "  batch, h, w, d = [x.value for x in pool1.get_shape()]    \n",
    "  flatten = tf.reshape(pool1, [-1, h*w*d])\n",
    "  print 'flatten', flatten\n",
    "\n",
    "  fc_weights = tf.Variable(tf.random_normal([h*w*d, 10], stddev=0.01))\n",
    "  fc_bias = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "  logits = tf.matmul(flatten, fc_weights) + fc_bias\n",
    "  print 'logits', logits\n",
    "\n",
    "  return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "* MNIST Example과 동일한 모델\n",
    "```python\n",
    "def build_model(images):\n",
    "    \n",
    "    x_image = tf.reshape(images, [-1, 28, 28, 1])\n",
    "    \n",
    "    # filter shape : w, h, in_channel, out_channel\n",
    "    conv1_filters = tf.Variable(tf.random_normal([3, 3, 1, 16], stddev=0.01))\n",
    "    conv1 = tf.nn.conv2d(x_image, conv1_filters, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    print 'conv1', conv1\n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    print 'pool1', pool1\n",
    "    \n",
    "    batch, h, w, d = [x.value for x in pool1.get_shape()]    \n",
    "    flatten = tf.reshape(pool1, [-1, h*w*d])\n",
    "    print 'flatten', flatten\n",
    "    \n",
    "    fc_weights = tf.Variable(tf.random_normal([h*w*d, 10], stddev=0.01))\n",
    "    fc_bias = tf.Variable(tf.random_normal([10]))\n",
    "    \n",
    "    logits = tf.matmul(flatten, fc_weights) + fc_bias\n",
    "    print 'logits', logits\n",
    "    \n",
    "    return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(lr): \n",
    "    return tf.train.GradientDescentOptimizer(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Graph 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/fashion/t10k-labels-idx1-ubyte.gz\n",
      "conv1 Tensor(\"Relu:0\", shape=(?, 28, 28, 16), dtype=float32)\n",
      "pool1 Tensor(\"MaxPool:0\", shape=(?, 14, 14, 16), dtype=float32)\n",
      "flatten Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n",
      "logits Tensor(\"add:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = get_data()\n",
    "\n",
    "images, labels = get_inputs()\n",
    "logits = build_model(images)\n",
    "\n",
    "loss = get_loss(logits, labels)\n",
    "optimizer = get_optimizer(lr=0.1)\n",
    "\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(logits)\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10\n",
    "\n",
    "NUM_BATCH_PER_EPOCH = int(fashion_mnist.train.images.shape[0]/float(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "* <span class=\"burk\">training코드를 완성해주세요.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Epoch, 0 Step : accuracy(0.1000), loss(3.1915)\n",
      "0 Epoch, 50 Step : accuracy(0.5417), loss(1.1643)\n",
      "0 Epoch, 100 Step : accuracy(0.6990), loss(0.8796)\n",
      "0 Epoch, 150 Step : accuracy(0.7231), loss(0.7555)\n",
      "0 Epoch, 200 Step : accuracy(0.7273), loss(0.7760)\n",
      "0 Epoch, 250 Step : accuracy(0.7610), loss(0.6837)\n",
      "0 Epoch, 300 Step : accuracy(0.7500), loss(0.6686)\n",
      "0 Epoch, 350 Step : accuracy(0.7553), loss(0.7217)\n",
      "0 Epoch, 400 Step : accuracy(0.7846), loss(0.6035)\n",
      "0 Epoch, 450 Step : accuracy(0.8080), loss(0.5580)\n",
      "0 Epoch, 500 Step : accuracy(0.8072), loss(0.5517)\n",
      "1 Epoch, 0 Step : accuracy(0.8017), loss(0.5721)\n",
      "1 Epoch, 50 Step : accuracy(0.8236), loss(0.5092)\n",
      "1 Epoch, 100 Step : accuracy(0.8124), loss(0.5316)\n",
      "1 Epoch, 150 Step : accuracy(0.8265), loss(0.5032)\n",
      "1 Epoch, 200 Step : accuracy(0.8003), loss(0.5798)\n",
      "1 Epoch, 250 Step : accuracy(0.8353), loss(0.4800)\n",
      "1 Epoch, 300 Step : accuracy(0.8285), loss(0.4832)\n",
      "1 Epoch, 350 Step : accuracy(0.8408), loss(0.4627)\n",
      "1 Epoch, 400 Step : accuracy(0.8061), loss(0.5137)\n",
      "1 Epoch, 450 Step : accuracy(0.8434), loss(0.4525)\n",
      "1 Epoch, 500 Step : accuracy(0.8323), loss(0.4640)\n",
      "2 Epoch, 0 Step : accuracy(0.8472), loss(0.4400)\n",
      "2 Epoch, 50 Step : accuracy(0.8440), loss(0.4382)\n",
      "2 Epoch, 100 Step : accuracy(0.8413), loss(0.4460)\n",
      "2 Epoch, 150 Step : accuracy(0.8364), loss(0.4506)\n",
      "2 Epoch, 200 Step : accuracy(0.8204), loss(0.4932)\n",
      "2 Epoch, 250 Step : accuracy(0.8502), loss(0.4299)\n",
      "2 Epoch, 300 Step : accuracy(0.8563), loss(0.4207)\n",
      "2 Epoch, 350 Step : accuracy(0.8407), loss(0.4368)\n",
      "2 Epoch, 400 Step : accuracy(0.8480), loss(0.4436)\n",
      "2 Epoch, 450 Step : accuracy(0.8470), loss(0.4365)\n",
      "2 Epoch, 500 Step : accuracy(0.8429), loss(0.4291)\n",
      "3 Epoch, 0 Step : accuracy(0.8470), loss(0.4249)\n",
      "3 Epoch, 50 Step : accuracy(0.8585), loss(0.4082)\n",
      "3 Epoch, 100 Step : accuracy(0.8490), loss(0.4257)\n",
      "3 Epoch, 150 Step : accuracy(0.8420), loss(0.4493)\n",
      "3 Epoch, 200 Step : accuracy(0.8625), loss(0.4034)\n",
      "3 Epoch, 250 Step : accuracy(0.8610), loss(0.3993)\n",
      "3 Epoch, 300 Step : accuracy(0.8635), loss(0.3912)\n",
      "3 Epoch, 350 Step : accuracy(0.8590), loss(0.4082)\n",
      "3 Epoch, 400 Step : accuracy(0.8528), loss(0.4182)\n",
      "3 Epoch, 450 Step : accuracy(0.8641), loss(0.3878)\n",
      "3 Epoch, 500 Step : accuracy(0.8484), loss(0.4230)\n",
      "4 Epoch, 0 Step : accuracy(0.8439), loss(0.4207)\n",
      "4 Epoch, 50 Step : accuracy(0.8556), loss(0.4063)\n",
      "4 Epoch, 100 Step : accuracy(0.8488), loss(0.4234)\n",
      "4 Epoch, 150 Step : accuracy(0.8666), loss(0.3864)\n",
      "4 Epoch, 200 Step : accuracy(0.8566), loss(0.3993)\n",
      "4 Epoch, 250 Step : accuracy(0.8592), loss(0.3941)\n",
      "4 Epoch, 300 Step : accuracy(0.8662), loss(0.3817)\n",
      "4 Epoch, 350 Step : accuracy(0.8663), loss(0.3784)\n",
      "4 Epoch, 400 Step : accuracy(0.8688), loss(0.3758)\n",
      "4 Epoch, 450 Step : accuracy(0.8707), loss(0.3755)\n",
      "4 Epoch, 500 Step : accuracy(0.8688), loss(0.3857)\n",
      "5 Epoch, 0 Step : accuracy(0.8655), loss(0.3792)\n",
      "5 Epoch, 50 Step : accuracy(0.8700), loss(0.3731)\n",
      "5 Epoch, 100 Step : accuracy(0.8674), loss(0.3774)\n",
      "5 Epoch, 150 Step : accuracy(0.8767), loss(0.3618)\n",
      "5 Epoch, 200 Step : accuracy(0.8726), loss(0.3703)\n",
      "5 Epoch, 250 Step : accuracy(0.8674), loss(0.3765)\n",
      "5 Epoch, 300 Step : accuracy(0.8674), loss(0.3725)\n",
      "5 Epoch, 350 Step : accuracy(0.8760), loss(0.3637)\n",
      "5 Epoch, 400 Step : accuracy(0.8606), loss(0.3923)\n",
      "5 Epoch, 450 Step : accuracy(0.8721), loss(0.3677)\n",
      "5 Epoch, 500 Step : accuracy(0.8735), loss(0.3639)\n",
      "6 Epoch, 0 Step : accuracy(0.8741), loss(0.3591)\n",
      "6 Epoch, 50 Step : accuracy(0.8744), loss(0.3591)\n",
      "6 Epoch, 100 Step : accuracy(0.8755), loss(0.3639)\n",
      "6 Epoch, 150 Step : accuracy(0.8704), loss(0.3692)\n",
      "6 Epoch, 200 Step : accuracy(0.8760), loss(0.3507)\n",
      "6 Epoch, 250 Step : accuracy(0.8766), loss(0.3517)\n",
      "6 Epoch, 300 Step : accuracy(0.8756), loss(0.3587)\n",
      "6 Epoch, 350 Step : accuracy(0.8771), loss(0.3492)\n",
      "6 Epoch, 400 Step : accuracy(0.8758), loss(0.3564)\n",
      "6 Epoch, 450 Step : accuracy(0.8717), loss(0.3636)\n",
      "6 Epoch, 500 Step : accuracy(0.8593), loss(0.3855)\n",
      "7 Epoch, 0 Step : accuracy(0.8762), loss(0.3559)\n",
      "7 Epoch, 50 Step : accuracy(0.8760), loss(0.3479)\n",
      "7 Epoch, 100 Step : accuracy(0.8777), loss(0.3490)\n",
      "7 Epoch, 150 Step : accuracy(0.8655), loss(0.3692)\n",
      "7 Epoch, 200 Step : accuracy(0.8790), loss(0.3475)\n",
      "7 Epoch, 250 Step : accuracy(0.8741), loss(0.3568)\n",
      "7 Epoch, 300 Step : accuracy(0.8690), loss(0.3766)\n",
      "7 Epoch, 350 Step : accuracy(0.8785), loss(0.3393)\n",
      "7 Epoch, 400 Step : accuracy(0.8769), loss(0.3446)\n",
      "7 Epoch, 450 Step : accuracy(0.8799), loss(0.3431)\n",
      "7 Epoch, 500 Step : accuracy(0.8779), loss(0.3434)\n",
      "8 Epoch, 0 Step : accuracy(0.8788), loss(0.3493)\n",
      "8 Epoch, 50 Step : accuracy(0.8823), loss(0.3362)\n",
      "8 Epoch, 100 Step : accuracy(0.8740), loss(0.3447)\n",
      "8 Epoch, 150 Step : accuracy(0.8790), loss(0.3431)\n",
      "8 Epoch, 200 Step : accuracy(0.8768), loss(0.3480)\n",
      "8 Epoch, 250 Step : accuracy(0.8766), loss(0.3483)\n",
      "8 Epoch, 300 Step : accuracy(0.8799), loss(0.3382)\n",
      "8 Epoch, 350 Step : accuracy(0.8673), loss(0.3672)\n",
      "8 Epoch, 400 Step : accuracy(0.8783), loss(0.3438)\n",
      "8 Epoch, 450 Step : accuracy(0.8834), loss(0.3364)\n",
      "8 Epoch, 500 Step : accuracy(0.8785), loss(0.3487)\n",
      "9 Epoch, 0 Step : accuracy(0.8785), loss(0.3464)\n",
      "9 Epoch, 50 Step : accuracy(0.8817), loss(0.3354)\n",
      "9 Epoch, 100 Step : accuracy(0.8792), loss(0.3330)\n",
      "9 Epoch, 150 Step : accuracy(0.8831), loss(0.3324)\n",
      "9 Epoch, 200 Step : accuracy(0.8830), loss(0.3358)\n",
      "9 Epoch, 250 Step : accuracy(0.8804), loss(0.3317)\n",
      "9 Epoch, 300 Step : accuracy(0.8824), loss(0.3351)\n",
      "9 Epoch, 350 Step : accuracy(0.8812), loss(0.3361)\n",
      "9 Epoch, 400 Step : accuracy(0.8813), loss(0.3335)\n",
      "9 Epoch, 450 Step : accuracy(0.8744), loss(0.3595)\n",
      "9 Epoch, 500 Step : accuracy(0.8807), loss(0.3381)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step in range(NUM_BATCH_PER_EPOCH):\n",
    "        # batch_data 읽어오기\n",
    "        batch_images, batch_labels = fashion_mnist.train.next_batch(BATCH_SIZE)\n",
    "        # train_op실행하기\n",
    "        sess.run(train_op, feed_dict={images:batch_images, labels:batch_labels})\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            _accuracy, _loss = sess.run([accuracy, loss], feed_dict = {images:fashion_mnist.test.images, labels:fashion_mnist.test.labels})\n",
    "            print '{} Epoch, {} Step : accuracy({:.4f}), loss({:.4f})'.format(epoch, step, _accuracy, _loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "```python\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step in range(NUM_BATCH_PER_EPOCH):\n",
    "        # batch_data 읽어오기\n",
    "        batch_images, batch_labels = fashion_mnist.train.next_batch(BATCH_SIZE)\n",
    "        # train_op실행하기\n",
    "        sess.run(train_op, feed_dict={images:batch_images, labels:batch_labels})\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            _accuracy, _loss = sess.run([accuracy, loss], feed_dict = {images:fashion_mnist.test.images, labels:fashion_mnist.test.labels})\n",
    "            print '{} Epoch, {} Step : accuracy({:.4f}), loss({:.4f})'.format(epoch, step, _accuracy, _loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
